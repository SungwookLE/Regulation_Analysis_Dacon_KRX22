{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Document summary.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**한국어 문장요약 블로그 **\n","\n","참고 : https://lovit.github.io/nlp/2019/05/01/krwordrank_sentence/"],"metadata":{"id":"7TuKKAr3FGR_"}},{"cell_type":"code","source":["!pip install krwordrank\n","!pip install soynlp"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6_AgHuxO4qOw","executionInfo":{"status":"ok","timestamp":1657718521329,"user_tz":-540,"elapsed":12487,"user":{"displayName":"Sungwook LE","userId":"10208372181641170821"}},"outputId":"c051c47f-b4da-4c51-cf5f-89f5bc4c7aa7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting krwordrank\n","  Downloading krwordrank-1.0.3-py3-none-any.whl (20 kB)\n","Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.7/dist-packages (from krwordrank) (1.0.2)\n","Requirement already satisfied: numpy>=1.18.4 in /usr/local/lib/python3.7/dist-packages (from krwordrank) (1.21.6)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from krwordrank) (1.5.4)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.1->krwordrank) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.1->krwordrank) (3.1.0)\n","Installing collected packages: krwordrank\n","Successfully installed krwordrank-1.0.3\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting soynlp\n","  Downloading soynlp-0.0.493-py3-none-any.whl (416 kB)\n","\u001b[K     |████████████████████████████████| 416 kB 3.6 MB/s \n","\u001b[?25hRequirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from soynlp) (1.0.2)\n","Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.7/dist-packages (from soynlp) (1.21.6)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from soynlp) (1.5.4)\n","Requirement already satisfied: psutil>=5.0.1 in /usr/local/lib/python3.7/dist-packages (from soynlp) (5.4.8)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->soynlp) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->soynlp) (3.1.0)\n","Installing collected packages: soynlp\n","Successfully installed soynlp-0.0.493\n"]}]},{"cell_type":"markdown","source":[""],"metadata":{"id":"_YXcAN-PE_GK"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"UqTWnO3gub3g"},"outputs":[],"source":["import os\n","from pickle import FALSE\n","import re\n","\n","import pandas as pd \n","import numpy as np\n","import math\n","import pprint \n","import datetime\n","from sklearn.model_selection import train_test_split\n","from functools import reduce\n","from typing import Text\n","from krwordrank.word import KRWordRank\n","from scipy.sparse import csr_matrix\n","from soynlp.tokenizer import MaxScoreTokenizer\n","\n","\n","# 크롤링으로 수집한 데이터를 전처리하여 필요한 필드를 추가하는 데이터\n","RAW_DATA_DIR = \"..//data//orgin_data//\"\n","PROCESSED_DATA_DIR = \"..//news_article//data//processed//\"\n","rawdata = \"extract_data.xlsx\"\n","\n","\n","class TextMing :\n","    def remove_unicode(input_str):\n","        # 유니코드 :  https://gist.github.com/Pusnow/aa865fa21f9557fa58d691a8b79f8a6d\n","        import unicodedata\n","        try:\n","            input_str = str(input_str)\n","            text = unicode(input_str, 'utf-8')\n","            print(text)\n","            \n","        except (TypeError, NameError): # unicode is a default on python 3 \n","            pass\n","        \n","\n","        text = unicodedata.normalize('NFKC', input_str)\n","        text = text.encode('utf-8','ignore')\n","        text = text.decode(\"utf-8\")\n","            \n","        return text\n","\n","    def clean_ko_sentences(sentences):\n","        sentence = TextMing.remove_unicode(sentences)\n","        # 정규 표현식 필터\n","        RE_FILTER = re.compile(r\"[#▲,<>!?\\\"'·:;=~()]\") # [] 안에 제거할 정규표현식을 추가한다\n","        # 특수기호 제거\n","        sentences = re.sub(RE_FILTER, \"\", sentence)\n","        sentences = re.sub(r\"[^A-Za-z0-9가-힣?!,¿]+\", \" \",sentences) \n","        sentences = re.sub(r\"\\s\",\" \", sentences)# \\n도 공백으로 대체해줌\n","        sentences = sentences.strip()  \n","        \n","        return str(sentences)\n"]},{"cell_type":"code","source":["\n","def Morph_ko_document(sentence_List):\n","    # KoNLPy 형태소분석기 설정 '\n","    from konlpy.tag import Hannanum\n","    hannanum = Hannanum()\n","\n","    nouns = []\n","\n","    for sentence in sentence_List:\n","        sentence = TextMing.clean_ko_documents(sentence)\n","        # print(sentence)\n","        nouns_list = hannanum.nouns(str(sentence))\n","        nouns.append(nouns_list)\n","        # nouns_list =  TextMing.clean_ko_words(nouns_list)\n","\n","    return nouns \n","\n","\n","def extract(self, docs, beta=0.85, max_iter=10, num_keywords=-1,\n","    num_rset=-1, vocabulary=None, bias=None, rset=None):\n","\n","    rank, graph = self.train(docs, beta, max_iter, vocabulary, bias)\n","\n","    lset = {self.int2token(idx)[0]:r for idx, r in rank.items() if self.int2token(idx)[1] == 'L'}\n","    if not rset:\n","        rset = {self.int2token(idx)[0]:r for idx, r in rank.items() if self.int2token(idx)[1] == 'R'}\n","\n","    if num_rset > 0:\n","        rset = {token:r for token, r in sorted(rset.items(), key=lambda x:-x[1])[:num_rset]}\n","\n","    keywords = self._select_keywords(lset, rset)\n","    keywords = self._filter_compounds(keywords)\n","    keywords = self._filter_subtokens(keywords)\n","\n","    if num_keywords > 0:\n","        keywords = {token:r for token, r in sorted(keywords.items(), key=lambda x:-x[1])[:num_keywords]}\n","    \n","    else : \n","        pass\n","\n","    return keywords, rank, graph\n","\n","\n","def make_vocab_score(keywords, scaling=None):\n","    if scaling is None:\n","        scaling = lambda x:math.sqrt(x)\n","    return {word:scaling(rank) for word, rank in keywords.items()}\n","\n","\n","def vectorize(sents, vocab_to_idx, tokenize):\n","    from scipy.sparse import csr_matrix\n","    from soynlp.tokenizer import MaxScoreTokenizer\n","    \n","    rows, cols, data = [], [], []\n","    for i, sent in enumerate(sents):\n","        terms = set(tokenize(sent))\n","        for term in terms:\n","            j = vocab_to_idx.get(term, -1)\n","            if j == -1:\n","                continue\n","            rows.append(i)\n","            cols.append(j)\n","            data.append(1)\n","\n","    n_docs = len(sents)\n","    n_terms = len(vocab_to_idx)\n","    return csr_matrix((data, (rows, cols)), shape=(n_docs, n_terms))\n","\n","\n","def select(x, keyvec, texts, initial_penalty, topk=10):\n","    from sklearn.metrics import pairwise_distances\n","\n","    dist = pairwise_distances(x, keyvec, metric='cosine').reshape(-1)\n","    dist = dist + initial_penalty\n","\n","    idxs = []\n","    for _ in range(topk):\n","        idx = dist.argmin()\n","        idxs.append(idx)\n","        dist[idx] += 2 # maximum distance of cosine is 2\n","        idx_all_distance = pairwise_distances(\n","            x, x[idx].reshape(1,-1), metric='cosine').reshape(-1)\n","        penalty = np.zeros(idx_all_distance.shape[0])\n","        penalty[np.where(idx_all_distance < diversity)[0]] = 2\n","        dist += penalty\n","    return [texts[idx] for idx in idxs]"],"metadata":{"id":"bTkvJwOy3bRf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from krwordrank.sentence import summarize_with_sentences\n","\n","penalty = lambda x:0 if (25 <= len(x) <= 80) else 1\n","stopwords = {'영화', '관람객', '너무', '정말', '진짜'}\n","texts  = []\n","\n","\n","keywords, sents = summarize_with_sentences(\n","      texts,\n","      penalty=penalty,\n","      stopwords = stopwords,\n","      diversity=0.7,\n","      num_keywords=100,\n","      num_keysents=10,\n","      scaling=lambda x:1,\n","      verbose=False,)"],"metadata":{"id":"-UMdLQZfDvTz","colab":{"base_uri":"https://localhost:8080/","height":345},"executionInfo":{"status":"error","timestamp":1657718548118,"user_tz":-540,"elapsed":496,"user":{"displayName":"Sungwook LE","userId":"10208372181641170821"}},"outputId":"09431f6b-2c46-4537-8af2-9968adbb69fe"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-708624c693df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0mnum_keysents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0mscaling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m       verbose=False,)\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/krwordrank/sentence/_sentence.py\u001b[0m in \u001b[0;36msummarize_with_sentences\u001b[0;34m(texts, num_keywords, num_keysents, diversity, stopwords, scaling, penalty, min_count, max_length, beta, max_iter, num_rset, verbose)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     keywords, rank, graph = wordrank_extractor.extract(texts,\n\u001b[0;32m--> 149\u001b[0;31m         beta, max_iter, num_keywords=num_keywords_, num_rset=num_rset)\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;31m# build tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/krwordrank/word/_word.py\u001b[0m in \u001b[0;36mextract\u001b[0;34m(self, docs, beta, max_iter, num_keywords, num_rset, vocabulary, bias, rset)\u001b[0m\n\u001b[1;32m    209\u001b[0m         \"\"\"\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mlset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint2token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mr\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint2token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'L'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/krwordrank/word/_word.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, docs, beta, max_iter, vocabulary, bias)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0msum_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     \u001b[0mnumber_of_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m                     )\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/krwordrank/graph/_rank.py\u001b[0m in \u001b[0;36mhits\u001b[0;34m(graph, beta, max_iter, bias, verbose, sum_weight, number_of_nodes, converge)\u001b[0m\n\u001b[1;32m     38\u001b[0m         raise ValueError(\n\u001b[1;32m     39\u001b[0m             \u001b[0;34m'The graph should consist of at least two nodes\\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0;34m'The node size of inserted graph is %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnumber_of_nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         )\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: ('The graph should consist of at least two nodes\\n', 'The node size of inserted graph is 0')"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"RDflDBxMQ2zb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#핵심 단어 추출\n","\n","from krwordrank.word import summarize_with_keywords\n","\n","keywords = summarize_with_keywords(texts, min_count=5, max_length=10,\n","    beta=0.85, max_iter=10, stopwords=stopwords, verbose=True)\n","keywords = summarize_with_keywords(texts)"],"metadata":{"id":"cBYon0HEFkKh"},"execution_count":null,"outputs":[]}]}